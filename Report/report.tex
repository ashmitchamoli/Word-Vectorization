\documentclass[a4paper,9pt]{report}

\usepackage{subfig}
\usepackage{float}
\usepackage[myheadings]{fullpage}
\usepackage{fancyhdr}
% \usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{bbold}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}

\pagestyle{fancy}
\fancyhf{}
\setlength\headheight{15pt}
\fancyhead[R]{Introduction to Natural Language Processing}

\usepackage[english]{babel}
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\begin{document}

\title{ \normalsize \textsc{\LARGE Word Vectorization}
		\\ [2.0cm]
		\HRule{0.5pt} \\
		\LARGE \textbf{\uppercase{Report}}
		\HRule{2pt} \\ [0.5cm]
		\normalsize \vspace*{3\baselineskip}}
        \date{ }

\author{Ashmit Chamoli\\ }

\maketitle
\section*{Introduction}
Word vectorization refers to the task of generating meaningful numerical representations for words. These are also known as distributional semantic algorithms.
In this repository, we explore 2 word vectorization techniques:
\begin{itemize}
    \item SVD
    \item Word2Vec (skip-gram with negative sampling)
\end{itemize}

\section*{Dataset}
We use the \href{https://iiitaphyd-my.sharepoint.com/:u:/g/personal/advaith_malladi_research_iiit_ac_in/EWjgIboHC19Ppq6Of9klUo4BlKgAqynxC0TRBURzQ0lEzA?e=tWZqY5}{News Classification Dataset} which contains labeled news snippets. 
First, we obtain the word embeddings using both SVD and Word2Vec techniques. 
Then we use these embeddings to train an LSTM classifier to classify the news snippets.

\section*{Experiments}
We train 6 embedding models, 3 each for SVD and Word2Vec, varying context size from 2 to 4 with an embedding size of 300 and k=3 for Word2Vec.
\\
We use the same classifier for all the 6 set of embeddings with the following hyperparameters:
\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|l|l|}
        \hline
        \textbf{Hidden Size} & \textbf{Num Layers} & \textbf{Bidirectional} & \textbf{Hidden Layers} & \textbf{Activation} \\ \hline
        256 & 3 & True & [128, 64] & tanh \\ \hline
    \end{tabular}
\end{table}

\section*{Analysis}
We observe that Word2Vec embeddings give better performance than SVD embeddings.
 
\subsection*{LM 1: Tokenization + 3-gram LM + Good-Turing Smoothing}
\subsubsection*{Pride and Prejudice}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Train} & {90651.12} \\
        \hline
        \textbf{Test} & {12722.59} \\
        \hline
    \end{tabular}
\end{table}
The high perplexity values on the test set indicate that the model is quite confused about it's prediction. The higher value of perplexity in the train set is because Good Turing smoothing assigns a very high probability to unseen n-grams which in turns results in probability of seen but low frequency n-grams to be extremely low. 
In the test set however, we see a lot of unseen n-grams for which the good turing model returns a very high probability.

\subsubsection*{Ulyess}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Train} & {215313.48} \\
        \hline
        \textbf{Test} & {16327.17} \\
        \hline
    \end{tabular}
\end{table}
Here similar trend is followed, except that the train perplexity is much higher than we see in Pride and Prejudice. This is because of a much larger vocabulary set and a larger dataset. This results in the probability of unseen n-grams to be even higher than was the case in Pride and Prejudice dataset.

\subsection*{LM 2: Tokenization + 3-gram LM + Linear Interpolation}
\subsubsection*{Pride and Prejudice}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Train} & {27.70} \\
        \hline
        \textbf{Test} & {813.61} \\
        \hline
    \end{tabular}
\end{table}


\subsubsection*{Ulyess}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Train} & {97.05} \\
        \hline
        \textbf{Test} & {2463.43} \\
        \hline
    \end{tabular}
\end{table}

The perplexity in the for the train sets is quite low, indicating that the model is quite sure of it's prediction. 
On the test sets however, the overall perplexity is much lower than we see in Good Turing, meaning that the model is performing better in this case.

The perplexity scores are always higher for the Ulyess dataset as compared to the Pride and Prejudice dataset. This might be because of the richer vocabulary in the Ulyess dataset.

The best performance we achieve is by LM2 on the Pride and Prejudice dataset.

\end{document}
