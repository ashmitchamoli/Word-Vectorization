{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "trainFileName = './data/News Classification Dataset/train.csv'\n",
    "testFileName = './data/News Classification Dataset/test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Embedding Models\n",
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data. Vocabulary size: 43605\n",
      "Found and loaded trained embeddings.\n"
     ]
    }
   ],
   "source": [
    "import word_vectorization.models.word2vec.Word2Vec as Word2Vec\n",
    "importlib.reload(Word2Vec)\n",
    "\n",
    "word2VecModel = Word2Vec.Word2Vec(2, trainFileName, embeddingSize=300, k=3)\n",
    "word2VecEmbeddings = word2VecModel.train(epochs=10, lr=0.005, batchSize=2**12, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data. Vocabulary size: 43605\n",
      "Embeddings not found. Starting training from scratch.\n",
      "Computed co-occurence matrix.\n",
      "Computed Partial Singular Value Decomposition of the co-occurence matrix.\n"
     ]
    }
   ],
   "source": [
    "import word_vectorization.models.svd.SVD as SVD\n",
    "importlib.reload(SVD)\n",
    "\n",
    "svdModel = SVD.SvdWordVectorizationModel(4, trainFileName, embeddingSize=300)\n",
    "svdEmbeddings = svdModel.train(retrain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data. Vocabulary size: 43605\n",
      "Embeddings not found. Starting training from scratch.\n",
      "Computed co-occurence matrix.\n",
      "Computed Partial Singular Value Decomposition of the co-occurence matrix.\n"
     ]
    }
   ],
   "source": [
    "svdModel = SVD.SvdWordVectorizationModel(2, trainFileName, embeddingSize=300)\n",
    "svdEmbeddings = svdModel.train(retrain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data. Vocabulary size: 43605\n",
      "Model already trained. Embeddings loaded.\n"
     ]
    }
   ],
   "source": [
    "svdModel = SVD.SvdWordVectorizationModel(2, trainFileName, embeddingSize=300)\n",
    "svdEmbeddings = svdModel.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data. Vocabulary size: 43605\n",
      "Embeddings not found. Starting training from scratch.\n",
      "Computed co-occurence matrix.\n",
      "Computed Partial Singular Value Decomposition of the co-occurence matrix.\n"
     ]
    }
   ],
   "source": [
    "svdModel = SVD.SvdWordVectorizationModel(3, trainFileName, embeddingSize=300)\n",
    "svdEmbeddings = svdModel.train(retrain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data. Vocabulary size: 43605\n",
      "Model already trained. Embeddings loaded.\n"
     ]
    }
   ],
   "source": [
    "svdModel = SVD.SvdWordVectorizationModel(3, trainFileName, embeddingSize=300)\n",
    "svdEmbeddings = svdModel.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data. Vocabulary size: 43605\n",
      "Model already trained. Embeddings loaded.\n"
     ]
    }
   ],
   "source": [
    "svdModel = SVD.SvdWordVectorizationModel(4, trainFileName, embeddingSize=300)\n",
    "svdEmbeddings = svdModel.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import word_vectorization.classification.LstmClassifier as LstmClassifier\n",
    "import word_vectorization.datasets.ClassificationDataset as ClassificationDataset\n",
    "importlib.reload(LstmClassifier)\n",
    "importlib.reload(ClassificationDataset)\n",
    "\n",
    "classifierHyperParams = {'hiddenSize': 256, 'numLayers': 3, 'bidirectional': True, 'hiddenLayers': [128, 64], 'activation': 'tanh'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "We will use the embeddings obtained from Word2Vec for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model not found or retrain flag is set. Starting training from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 1273/3750 [09:32<15:41,  2.63it/s]"
     ]
    }
   ],
   "source": [
    "word2VecClassifier = LstmClassifier.SentenceClassifier(trainFileName,\n",
    "                                                       word2VecModel,\n",
    "                                                       **classifierHyperParams)\n",
    "word2VecClassifier.train(epochs=4,\n",
    "                         lr=0.001,\n",
    "                         batchSize=32,\n",
    "                         verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n",
    "We will use the embeddings obtained from SVD for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model not found or retrain flag is set. Starting training from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3750/3750 [22:25<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 | Loss: 1.389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3750/3750 [24:09<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4 | Loss: 1.389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3750/3750 [23:48<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4 | Loss: 1.389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3750/3750 [23:58<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/4 | Loss: 1.389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "svdClassifier = LstmClassifier.SentenceClassifier(trainFileName,\n",
    "                                                  svdModel,\n",
    "                                                  **classifierHyperParams)\n",
    "svdClassifier.train(epochs=4,\n",
    "                    lr=0.001,\n",
    "                    batchSize=32,\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ClassificationDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainDataset \u001b[38;5;241m=\u001b[39m \u001b[43mClassificationDataset\u001b[49m\u001b[38;5;241m.\u001b[39mClassificationDataset(trainFileName, word2VecModel\u001b[38;5;241m.\u001b[39mwordIndices)\n\u001b[1;32m      2\u001b[0m testDataset \u001b[38;5;241m=\u001b[39m ClassificationDataset\u001b[38;5;241m.\u001b[39mClassificationDataset(testFileName, word2VecModel\u001b[38;5;241m.\u001b[39mwordIndices)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ClassificationDataset' is not defined"
     ]
    }
   ],
   "source": [
    "trainDataset = ClassificationDataset.ClassificationDataset(trainFileName, word2VecModel.wordIndices)\n",
    "testDataset = ClassificationDataset.ClassificationDataset(testFileName, word2VecModel.wordIndices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Classifier\n",
    "### Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91      1900\n",
      "           1       0.96      0.97      0.96      1900\n",
      "           2       0.87      0.85      0.86      1900\n",
      "           3       0.84      0.89      0.87      1900\n",
      "\n",
      "    accuracy                           0.90      7600\n",
      "   macro avg       0.90      0.90      0.90      7600\n",
      "weighted avg       0.90      0.90      0.90      7600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word2VecScores = word2VecClassifier.evaluate(testDataset)\n",
    "print(word2VecScores['Report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train set evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1900\n",
      "           1       0.00      0.00      0.00      1900\n",
      "           2       0.00      0.00      0.00      1900\n",
      "           3       0.25      1.00      0.40      1900\n",
      "\n",
      "    accuracy                           0.25      7600\n",
      "   macro avg       0.06      0.25      0.10      7600\n",
      "weighted avg       0.06      0.25      0.10      7600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svdScores = svdClassifier.evaluate(testDataset)\n",
    "print(svdScores['Report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "We will try different context sizes for each model.\n",
    "\n",
    "## Training word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextSizes = [ 2, 3, 4, 5 ]\n",
    "\n",
    "word2VecModels = []\n",
    "svdModels = []\n",
    "\n",
    "for contextSize in contextSizes:\n",
    "    w2vModel = Word2Vec.Word2Vec(contextSize, trainFileName, embeddingSize=300, k=3)\n",
    "    w2vEmbeddings = w2vModel.train(epochs=10, lr=0.005, batchSize=2**12, verbose=True)\n",
    "    word2VecModels.append(w2vModel)\n",
    "\n",
    "    svdModel = SVD.SvdWordVectorizationModel(3, trainFileName, embeddingSize=300)\n",
    "    svdEmbeddings = svdModel.train()\n",
    "    svdModels.append(svdModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Sentence Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFileName = './data/News Classification Dataset/train.csv'\n",
    "w2vScores = {} # context size : scores\n",
    "svdScores = {} # context size : scores\n",
    "\n",
    "# compare the models\n",
    "for w2vModel in word2VecModels:\n",
    "    classifier = LstmClassifier.SentenceClassifier(trainFileName,\n",
    "                                                   w2vModel,\n",
    "                                                   **classifierHyperParams)\n",
    "    classifier.train(epochs=5, lr=0.001, batchSize=32, verbose=True)\n",
    "\n",
    "    w2vScores[w2vModel.contextSize] = classifier.evaluate(testDataset)\n",
    "\n",
    "for svdModel in svdModels:\n",
    "    classifier = LstmClassifier.SentenceClassifier(trainFileName,\n",
    "                                                   svdModel,\n",
    "                                                   **classifierHyperParams)\n",
    "    classifier.train(epochs=5, lr=0.001, batchSize=32, verbose=True)\n",
    "\n",
    "    svdScores[svdModel.contextSize] = classifier.evaluate(testDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank the models\n",
    "from tabulate import tabulate\n",
    "\n",
    "# sort according to f1 scores\n",
    "sortedW2vScores = sorted(w2vScores.items(), key=lambda x: x[1]['F1'], reverse=True)\n",
    "sortedSvdScores = sorted(svdScores.items(), key=lambda x: x[1]['F1'], reverse=True)\n",
    "\n",
    "print(tabulate(sortedW2vScores, headers=['Context Size', 'Scores']))\n",
    "print(tabulate(sortedSvdScores, headers=['Context Size', 'Scores']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
