{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pickle\n",
    "\n",
    "trainFileName = './data/News Classification Dataset/train.csv'\n",
    "testFileName = './data/News Classification Dataset/test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Embedding Models\n",
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data. Vocabulary size: 43605\n",
      "Found and loaded trained embeddings.\n"
     ]
    }
   ],
   "source": [
    "import word_vectorization.models.word2vec.Word2Vec as Word2Vec\n",
    "importlib.reload(Word2Vec)\n",
    "\n",
    "word2VecModel = Word2Vec.Word2Vec(2, trainFileName, embeddingSize=300, k=3)\n",
    "word2VecEmbeddings = word2VecModel.train(epochs=10, lr=0.005, batchSize=2**12, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data. Vocabulary size: 43605\n",
      "Embeddings not found. Starting training from scratch.\n",
      "Computed co-occurence matrix.\n",
      "Computed Partial Singular Value Decomposition of the co-occurence matrix.\n"
     ]
    }
   ],
   "source": [
    "import word_vectorization.models.svd.SVD as SVD\n",
    "importlib.reload(SVD)\n",
    "\n",
    "svdModel = SVD.SvdWordVectorizationModel(5, trainFileName, embeddingSize=300)\n",
    "svdEmbeddings = svdModel.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import word_vectorization.classification.LstmClassifier as LstmClassifier\n",
    "import word_vectorization.datasets.ClassificationDataset as ClassificationDataset\n",
    "importlib.reload(LstmClassifier)\n",
    "importlib.reload(ClassificationDataset)\n",
    "\n",
    "classifierHyperParams = {'hiddenSize': 256, 'numLayers': 3, 'bidirectional': True, 'hiddenLayers': [128, 64], 'activation': 'tanh'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "We will use the embeddings obtained from Word2Vec for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model not found or retrain flag is set. Starting training from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3750/3750 [34:48<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 | Loss: 0.867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3750/3750 [29:08<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4 | Loss: 0.359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3750/3750 [24:48<00:00,  2.52it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4 | Loss: 0.248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3750/3750 [23:04<00:00,  2.71it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/4 | Loss: 0.188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word2VecClassifier = LstmClassifier.SentenceClassifier(trainFileName,\n",
    "                                                       word2VecModel,\n",
    "                                                       **classifierHyperParams)\n",
    "word2VecClassifier.train(epochs=4,\n",
    "                         lr=0.001,\n",
    "                         batchSize=32,\n",
    "                         verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n",
    "We will use the embeddings obtained from SVD for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and loaded trained model.\n"
     ]
    }
   ],
   "source": [
    "svdClassifier = LstmClassifier.SentenceClassifier(trainFileName,\n",
    "                                                  svdModel,\n",
    "                                                  **classifierHyperParams)\n",
    "svdClassifier.train(epochs=10,\n",
    "                    lr=0.001,\n",
    "                    batchSize=32,\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = ClassificationDataset.ClassificationDataset(trainFileName, word2VecModel.wordIndices)\n",
    "testDataset = ClassificationDataset.ClassificationDataset(testFileName, word2VecModel.wordIndices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Classifier\n",
    "### Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90      1900\n",
      "           1       0.94      0.97      0.96      1900\n",
      "           2       0.90      0.84      0.87      1900\n",
      "           3       0.89      0.89      0.89      1900\n",
      "\n",
      "    accuracy                           0.91      7600\n",
      "   macro avg       0.91      0.91      0.91      7600\n",
      "weighted avg       0.91      0.91      0.91      7600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word2VecScores = word2VecClassifier.evaluate(testDataset)\n",
    "print(word2VecScores['Report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train set evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1900\n",
      "           1       0.00      0.00      0.00      1900\n",
      "           2       0.00      0.00      0.00      1900\n",
      "           3       0.25      1.00      0.40      1900\n",
      "\n",
      "    accuracy                           0.25      7600\n",
      "   macro avg       0.06      0.25      0.10      7600\n",
      "weighted avg       0.06      0.25      0.10      7600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svdScores = svdClassifier.evaluate(testDataset)\n",
    "print(svdScores['Report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "We will try different context sizes for each model.\n",
    "\n",
    "## Training word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data. Vocabulary size: 43605\n",
      "Found and loaded trained embeddings.\n",
      "Loaded training data. Vocabulary size: 43605\n",
      "Embeddings not found. Starting training from scratch.\n",
      "Computed co-occurence matrix.\n",
      "Computed Partial Singular Value Decomposition of the co-occurence matrix.\n",
      "Loaded training data. Vocabulary size: 43605\n",
      "Found and loaded trained embeddings.\n",
      "Loaded training data. Vocabulary size: 43605\n",
      "Embeddings not found. Starting training from scratch.\n",
      "Computed co-occurence matrix.\n",
      "Computed Partial Singular Value Decomposition of the co-occurence matrix.\n",
      "Loaded training data. Vocabulary size: 43605\n",
      "Found and loaded trained embeddings.\n",
      "Loaded training data. Vocabulary size: 43605\n",
      "Embeddings not found. Starting training from scratch.\n",
      "Computed co-occurence matrix.\n",
      "Computed Partial Singular Value Decomposition of the co-occurence matrix.\n"
     ]
    }
   ],
   "source": [
    "contextSizes = [ 2, 3, 4 ]\n",
    "\n",
    "word2VecModels = []\n",
    "svdModels = []\n",
    "\n",
    "trainFileName = './data/News Classification Dataset/train.csv'\n",
    "\n",
    "for contextSize in contextSizes:\n",
    "    w2vModel = Word2Vec.Word2Vec(contextSize, trainFileName, embeddingSize=300, k=3)\n",
    "    w2vEmbeddings = w2vModel.train(epochs=10, lr=0.005, batchSize=2**12, verbose=True)\n",
    "    word2VecModels.append(w2vModel)\n",
    "\n",
    "    svdModel = SVD.SvdWordVectorizationModel(contextSize, trainFileName, embeddingSize=100)\n",
    "    svdEmbeddings = svdModel.train()\n",
    "    svdModels.append(svdModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Sentence Classifiers\n",
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and loaded trained model.\n",
      "Found and loaded trained model.\n",
      "Found and loaded trained model.\n"
     ]
    }
   ],
   "source": [
    "w2vScores = {} # context size : scores\n",
    "for w2vModel in word2VecModels:\n",
    "    classifier = LstmClassifier.SentenceClassifier(trainFileName,\n",
    "                                                   w2vModel,\n",
    "                                                   **classifierHyperParams)\n",
    "    classifier.train(epochs=4, lr=0.001, batchSize=32, verbose=True)\n",
    "\n",
    "    w2vScores[w2vModel.contextSize] = classifier.evaluate(testDataset)\n",
    "\n",
    "    pickle.dump(w2vScores, open('w2v_scores.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "svdScores = {} # context size : scores\n",
    "trainFileName = './data/W2vData/train.csv'\n",
    "def trainSvdModel(svdModel):\n",
    "    classifier = LstmClassifier.SentenceClassifier(trainFileName,\n",
    "                                                   svdModel,\n",
    "                                                   **classifierHyperParams)\n",
    "    classifier.train(epochs=15, lr=0.005, batchSize=32, verbose=True)\n",
    "\n",
    "    svdScores[svdModel.contextSize] = classifier.evaluate(testDataset)\n",
    "\n",
    "    pickle.dump(svdScores, open('svd_scores.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model not found or retrain flag is set. Starting training from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [02:37<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Loss: 1.389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [02:31<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 | Loss: 1.387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [02:34<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 | Loss: 1.387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [02:35<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 | Loss: 1.387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [02:34<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 | Loss: 1.387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [02:31<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 | Loss: 1.387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [02:32<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 | Loss: 1.387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [02:32<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 | Loss: 1.388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [02:37<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 | Loss: 1.387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [02:30<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 | Loss: 1.386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [02:35<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 | Loss: 1.388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [02:31<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 | Loss: 1.387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [02:30<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 | Loss: 1.389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [02:25<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 | Loss: 1.386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [02:25<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 | Loss: 1.387\n"
     ]
    }
   ],
   "source": [
    "trainSvdModel(svdModels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSvdModel(svdModels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSvdModel(svdModels[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LstmClassifier.SentenceClassifier(trainFileName,\n",
    "                                               svdModels[0],\n",
    "                                               **classifierHyperParams)\n",
    "classifier.train(epochs=10, lr=0.001, batchSize=32, verbose=True)\n",
    "\n",
    "svdScores[2] = classifier.evaluate(testDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒════════════════╤════════════╤══════════╤═════════════╤══════════╕\n",
      "│   Context Size │   Accuracy │       F1 │   Precision │   Recall │\n",
      "╞════════════════╪════════════╪══════════╪═════════════╪══════════╡\n",
      "│              3 │   0.911184 │ 0.911222 │    0.911921 │ 0.911184 │\n",
      "├────────────────┼────────────┼──────────┼─────────────┼──────────┤\n",
      "│              2 │   0.905921 │ 0.905408 │    0.905792 │ 0.905921 │\n",
      "├────────────────┼────────────┼──────────┼─────────────┼──────────┤\n",
      "│              4 │   0.861579 │ 0.861858 │    0.875226 │ 0.861579 │\n",
      "╘════════════════╧════════════╧══════════╧═════════════╧══════════╛\n",
      "╒════════════════╤════════════╤══════╤═════════════╤══════════╕\n",
      "│   Context Size │   Accuracy │   F1 │   Precision │   Recall │\n",
      "╞════════════════╪════════════╪══════╪═════════════╪══════════╡\n",
      "│              2 │       0.25 │  0.1 │      0.0625 │     0.25 │\n",
      "╘════════════════╧════════════╧══════╧═════════════╧══════════╛\n"
     ]
    }
   ],
   "source": [
    "# rank the models\n",
    "from tabulate import tabulate\n",
    "\n",
    "word2VecScores = pickle.load(open('w2v_scores.pkl', 'rb'))\n",
    "svdScores = pickle.load(open('svd_scores.pkl', 'rb'))\n",
    "\n",
    "# sort according to f1 scores\n",
    "sortedW2vScores = sorted(word2VecScores.items(), key=lambda x: x[1]['F1'], reverse=True)\n",
    "sortedSvdScores = sorted(svdScores.items(), key=lambda x: x[1]['F1'], reverse=True)\n",
    "\n",
    "displayMetrics = 'Accuracy', 'F1', 'Precision', 'Recall'\n",
    "print(tabulate( (( (key,) + tuple(scores[metric] for metric in scores if metric in displayMetrics)) for key, scores in sortedW2vScores), headers=('Context Size',) + displayMetrics, tablefmt=\"fancy_grid\"))\n",
    "print(tabulate( (( (key,) + tuple(scores[metric] for metric in scores if metric in displayMetrics)) for key, scores in sortedSvdScores), headers=('Context Size',) + displayMetrics, tablefmt=\"fancy_grid\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
