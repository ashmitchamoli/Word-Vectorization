{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "trainFileName = './data/News Classification Dataset/train.csv'\n",
    "testFileName = './data/News Classification Dataset/test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Embedding Models\n",
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data. Vocabulary size: 43605\n",
      "Found and loaded trained embeddings.\n"
     ]
    }
   ],
   "source": [
    "import word_vectorization.models.word2vec.Word2Vec as Word2Vec\n",
    "importlib.reload(Word2Vec)\n",
    "\n",
    "word2VecModel = Word2Vec.Word2Vec(2, trainFileName, embeddingSize=300, k=3)\n",
    "word2VecEmbeddings = word2VecModel.train(epochs=10, lr=0.005, batchSize=2**12, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data. Vocabulary size: 43605\n",
      "Model already trained. Embeddings loaded.\n"
     ]
    }
   ],
   "source": [
    "import word_vectorization.models.svd.SVD as SVD\n",
    "importlib.reload(SVD)\n",
    "\n",
    "svdModel = SVD.SvdWordVectorizationModel(3, trainFileName, embeddingSize=300)\n",
    "svdEmbeddings = svdModel.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import word_vectorization.classification.LstmClassifier as LstmClassifier\n",
    "import word_vectorization.datasets.ClassificationDataset as ClassificationDataset\n",
    "importlib.reload(LstmClassifier)\n",
    "importlib.reload(ClassificationDataset)\n",
    "\n",
    "classifierHyperParams = {'hiddenSize': 256, 'numLayers': 3, 'bidirectional': True, 'hiddenLayers': [128, 64], 'activation': 'tanh'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "We will use the embeddings obtained from Word2Vec for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model not found or retrain flag is set. Starting training from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3750/3750 [34:48<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 | Loss: 0.867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3750/3750 [29:08<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4 | Loss: 0.359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 793/3750 [05:37<23:02,  2.14it/s]"
     ]
    }
   ],
   "source": [
    "word2VecClassifier = LstmClassifier.SentenceClassifier(trainFileName,\n",
    "                                                       word2VecModel,\n",
    "                                                       **classifierHyperParams)\n",
    "word2VecClassifier.train(epochs=4,\n",
    "                         lr=0.001,\n",
    "                         batchSize=32,\n",
    "                         verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n",
    "We will use the embeddings obtained from SVD for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model not found or retrain flag is set. Starting training from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashmitchamoli/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Training:  17%|█▋        | 645/3750 [07:04<34:04,  1.52it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m svdClassifier \u001b[38;5;241m=\u001b[39m LstmClassifier\u001b[38;5;241m.\u001b[39mSentenceClassifier(trainFileName,\n\u001b[1;32m      2\u001b[0m                                                   svdModel,\n\u001b[1;32m      3\u001b[0m                                                   \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclassifierHyperParams)\n\u001b[0;32m----> 4\u001b[0m \u001b[43msvdClassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/IIITH/III-II/INLP/Assignments/Word-Vectorization/word_vectorization/classification/LstmClassifier.py:51\u001b[0m, in \u001b[0;36mSentenceClassifier.train\u001b[0;34m(self, epochs, lr, batchSize, verbose, retrain)\u001b[0m\n\u001b[1;32m     48\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n\u001b[1;32m     50\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, y)\n\u001b[0;32m---> 51\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     54\u001b[0m runningLoss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "svdClassifier = LstmClassifier.SentenceClassifier(trainFileName,\n",
    "                                                  svdModel,\n",
    "                                                  **classifierHyperParams)\n",
    "svdClassifier.train(epochs=4,\n",
    "                    lr=0.001,\n",
    "                    batchSize=32,\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ClassificationDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainDataset \u001b[38;5;241m=\u001b[39m \u001b[43mClassificationDataset\u001b[49m\u001b[38;5;241m.\u001b[39mClassificationDataset(trainFileName, word2VecModel\u001b[38;5;241m.\u001b[39mwordIndices)\n\u001b[1;32m      2\u001b[0m testDataset \u001b[38;5;241m=\u001b[39m ClassificationDataset\u001b[38;5;241m.\u001b[39mClassificationDataset(testFileName, word2VecModel\u001b[38;5;241m.\u001b[39mwordIndices)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ClassificationDataset' is not defined"
     ]
    }
   ],
   "source": [
    "trainDataset = ClassificationDataset.ClassificationDataset(trainFileName, word2VecModel.wordIndices)\n",
    "testDataset = ClassificationDataset.ClassificationDataset(testFileName, word2VecModel.wordIndices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Classifier\n",
    "### Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91      1900\n",
      "           1       0.96      0.97      0.96      1900\n",
      "           2       0.87      0.85      0.86      1900\n",
      "           3       0.84      0.89      0.87      1900\n",
      "\n",
      "    accuracy                           0.90      7600\n",
      "   macro avg       0.90      0.90      0.90      7600\n",
      "weighted avg       0.90      0.90      0.90      7600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word2VecScores = word2VecClassifier.evaluate(testDataset)\n",
    "print(word2VecScores['Report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train set evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1900\n",
      "           1       0.00      0.00      0.00      1900\n",
      "           2       0.00      0.00      0.00      1900\n",
      "           3       0.25      1.00      0.40      1900\n",
      "\n",
      "    accuracy                           0.25      7600\n",
      "   macro avg       0.06      0.25      0.10      7600\n",
      "weighted avg       0.06      0.25      0.10      7600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svdScores = svdClassifier.evaluate(testDataset)\n",
    "print(svdScores['Report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "We will try different context sizes for each model.\n",
    "\n",
    "## Training word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextSizes = [ 2, 3, 4, 5 ]\n",
    "\n",
    "word2VecModels = []\n",
    "svdModels = []\n",
    "\n",
    "for contextSize in contextSizes:\n",
    "    w2vModel = Word2Vec.Word2Vec(contextSize, trainFileName, embeddingSize=300, k=3)\n",
    "    w2vEmbeddings = w2vModel.train(epochs=10, lr=0.005, batchSize=2**12, verbose=True)\n",
    "    word2VecModels.append(w2vModel)\n",
    "\n",
    "    svdModel = SVD.SvdWordVectorizationModel(3, trainFileName, embeddingSize=300)\n",
    "    svdEmbeddings = svdModel.train()\n",
    "    svdModels.append(svdModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Sentence Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFileName = './data/News Classification Dataset/train.csv'\n",
    "w2vScores = {} # context size : scores\n",
    "svdScores = {} # context size : scores\n",
    "\n",
    "# compare the models\n",
    "for w2vModel in word2VecModels:\n",
    "    classifier = LstmClassifier.SentenceClassifier(trainFileName,\n",
    "                                                   w2vModel,\n",
    "                                                   **classifierHyperParams)\n",
    "    classifier.train(epochs=5, lr=0.001, batchSize=32, verbose=True)\n",
    "\n",
    "    w2vScores[w2vModel.contextSize] = classifier.evaluate(testDataset)\n",
    "\n",
    "for svdModel in svdModels:\n",
    "    classifier = LstmClassifier.SentenceClassifier(trainFileName,\n",
    "                                                   svdModel,\n",
    "                                                   **classifierHyperParams)\n",
    "    classifier.train(epochs=5, lr=0.001, batchSize=32, verbose=True)\n",
    "\n",
    "    svdScores[svdModel.contextSize] = classifier.evaluate(testDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank the models\n",
    "from tabulate import tabulate\n",
    "\n",
    "# sort according to f1 scores\n",
    "sortedW2vScores = sorted(w2vScores.items(), key=lambda x: x[1]['F1'], reverse=True)\n",
    "sortedSvdScores = sorted(svdScores.items(), key=lambda x: x[1]['F1'], reverse=True)\n",
    "\n",
    "print(tabulate(sortedW2vScores, headers=['Context Size', 'Scores']))\n",
    "print(tabulate(sortedSvdScores, headers=['Context Size', 'Scores']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
